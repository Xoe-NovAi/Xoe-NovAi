# Xoe-NovAi Phase 1 v0.1.2 - Production-Ready Code Review

**Date**: October 18, 2025  
**Review Scope**: 45 files across config, core modules, dockerization, APIs, and tests  
**Overall Assessment**: 72% production-ready (up from ~40% at project start)  
**Critical Path to PR-Ready**: ~3-4 weeks of focused work

---

## Executive Summary: Critical Issues

| Category | Count | Severity | Blocker |
|----------|-------|----------|---------|
| Architecture issues | 8 | Critical | Yes |
| Code quality gaps | 12 | Major | Partial |
| Testing coverage | 5 | Major | Yes |
| Documentation gaps | 4 | Minor | No |

**Immediate Action Required**: Fix 8 architecture issues before any deployment attempt.

---

## I. CRITICAL ARCHITECTURE ISSUES (Blockers)

### 1. **Broken Import Chain** (CRITICAL)

**Problem**: Circular imports and missing path resolution

**Affected Files**:
- `main.py:31` imports `from logging_config import setup_logging`
- `logging_config.py` imports `from config_loader import get_config_value`
- `config_loader.py` has no issues, but entrypoint assumes it's in `sys.path`

**Current State**:
```python
# main.py - WRONG
from logging_config import setup_logging  # Fails: module not in path
from dependencies import get_llm  # Fails

# config_loader.py is called as:
CONFIG = load_config()  # Works inside Docker, fails in tests
```

**Impact**: Services won't start. `docker compose up` will fail with `ModuleNotFoundError`.

**Fix Required**:
```python
# ALL app modules must use absolute imports from app directory
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from config_loader import load_config
from logging_config import setup_logging
from dependencies import get_llm
```

**Validation**:
```bash
python3 -m pytest app/XNAi_rag_app/tests/ -v
docker compose build --no-cache 2>&1 | grep -i import
```

---

### 2. **`logging_config.py` Corrupted Structure** (CRITICAL)

**Problem**: Line 410 onwards contains malformed method definition inside class method

**Current Code** (lines 408-430):
```python
def log_token_generation(
    self,
    tokens: int,
    duration_s: float,
    model: str = "gemma-3-4b"
):
    """
    Log token generation performance.
    
    Args:
        tokens: Number of tokens generated
        duration_s: Time taken in seconds
        model: Model name
    """
    tokens_per_second = tokens / duration_s if duration_s > 0 else 0
    
    self.logger.info(
        f"Query {'succeeded' if success else 'failed'}",  # ERROR: 'success' undefined!
        extra={
            "operation": "query_processing",
            # ... incomplete structure
```

**Issues**:
- References undefined `success` variable
- Method signature doesn't match body
- Missing error parameter in signature

**Impact**: `PerformanceLogger` class is broken. Any code calling `perf_logger.log_token_generation()` will crash.

**Fix**: Replace entire section (lines 397-460) with corrected methods:

```python
def log_token_generation(self, tokens: int, duration_s: float, model: str = "gemma-3-4b"):
    """Log token generation performance."""
    tokens_per_second = tokens / duration_s if duration_s > 0 else 0
    self.logger.info(
        "Token generation completed",
        extra={
            "operation": "token_generation",
            "model": model,
            "tokens": tokens,
            "duration_s": round(duration_s, 3),
            "tokens_per_second": round(tokens_per_second, 2),
            "target_min": CONFIG['performance']['token_rate_min'],
            "target_max": CONFIG['performance']['token_rate_max'],
        }
    )

def log_crawl_operation(self, source: str, items: int, duration_s: float, 
                       success: bool = True, error: str = None):
    """Log crawler operation (NEW v0.1.2)."""
    items_per_hour = (items / duration_s * 3600) if duration_s > 0 else 0
    self.logger.info(
        f"Crawl {'completed' if success else 'failed'}: {source}",
        extra={
            "operation": "crawl",
            "source": source,
            "items": items,
            "duration_s": round(duration_s, 2),
            "items_per_hour": round(items_per_hour, 1),
            "success": success,
            "error": error,
        }
    )
```

---

### 3. **`config.toml` TOML Parse Error** (CRITICAL)

**Problem**: Lines 1-13 use bare text instead of TOML comments (missing `#`)

**Current** (lines 1-13):
```toml
============================================================================
Xoe-NovAi Phase 1 v0.1.2 Application Configuration
============================================================================
Purpose: Centralized application-level settings...
```

**Impact**: `toml.load('config.toml')` fails with `TomlDecodeError`. Stack won't initialize.

**Fix**: Add `#` to all comment lines:
```toml
# ============================================================================
# Xoe-NovAi Phase 1 v0.1.2 Application Configuration
# ============================================================================
# Purpose: Centralized application-level settings...
```

**Validation**:
```bash
python3 -c "import toml; toml.load('config.toml')" && echo "âœ“ Valid"
```

---

### 4. **Missing Module: `chainlit_app.py`** (CRITICAL)

**Problem**: `docker-compose.yml` line 136 runs `chainlit run chainlit_app.py`, but only `app.py` exists.

**Current**:
```yaml
CMD ["chainlit", "run", "XNAi_rag_app/chainlit_app.py", ...]
```

**Fix**:
```bash
cd app/XNAi_rag_app
mv app.py chainlit_app.py
```

Or update Dockerfile.chainlit:
```dockerfile
CMD ["chainlit", "run", "XNAi_rag_app/app.py", "--host", "0.0.0.0", "--port", "8001"]
```

---

### 5. **`healthcheck.py` Missing Import** (CRITICAL)

**Problem**: `check_crawler()` function (line 296) calls `get_curator()`, but it's not imported.

**Current** (line 23-24):
```python
from dependencies import get_llm, get_embeddings, get_vectorstore
# Missing: get_curator
```

**Fix**:
```python
from dependencies import get_llm, get_embeddings, get_vectorstore, get_curator
```

**Impact**: Health check crashes when crawler component is queried.

---

### 6. **`dependencies.py` - `get_curator()` Signature Mismatch** (MAJOR)

**Problem**: `get_curator()` is supposed to return a crawler instance, but code pattern suggests it should return a module.

**Current** (lines 435-470):
```python
def get_curator(cache_dir: Optional[str] = None, **kwargs) -> Any:
    """Initialize CrawlModule... Returns: Initialized crawler instance"""
    try:
        import crawl4ai
        crawler = WebCrawler(...)
        return crawler
    except ImportError:
        raise ImportError("CrawlModule requires...")
```

**Problem**: `crawl4ai.WebCrawler` is the external library. Your `crawl.py` is the wrapper. Mismatch between what's documented and what's implemented.

**Fix**: Clarify the architecture:

Option A (Recommended): Return the crawl module functions:
```python
def get_curator(cache_dir: Optional[str] = None, **kwargs) -> Any:
    """Get CrawlModule wrapper with curation functions."""
    try:
        sys.path.insert(0, '/app/XNAi_rag_app')
        import crawl
        logger.info("CrawlModule functions loaded")
        return crawl
    except ImportError as e:
        logger.error("crawl.py not found")
        raise ImportError("Requires crawl.py in app/XNAi_rag_app/") from e
```

Option B: Initialize and return crawl4ai wrapper:
```python
def get_curator(cache_dir: Optional[str] = None, **kwargs) -> Any:
    """Initialize crawl4ai WebCrawler."""
    try:
        from crawl4ai import WebCrawler
        crawler = WebCrawler(n_threads=6)
        crawler.warmup()
        return crawler
    except ImportError:
        raise
```

**Choose Option A** (aligns with your architecture of having `crawl.py` as abstraction layer).

---

### 7. **`main.py` and `app.py` are Duplicated/Conflicting** (MAJOR)

**Problem**: Two entry points for FastAPI and Chainlit, but unclear which is which.

**Current**:
- `main.py` (333 lines) - Appears to be FastAPI with query/stream endpoints
- `app.py` (391 lines) - Appears to be Chainlit UI

**Confusion**: 
- `docker-compose.yml` line 129: `CMD ["uvicorn", "main:app", ...]` (correct for FastAPI)
- `docker-compose.yml` line 136: `CMD ["chainlit", "run", "chainlit_app.py", ...]` (expects renamed `app.py`)

**Fix**:
```bash
# In app/XNAi_rag_app/:
mv app.py chainlit_app.py  # Now it matches docker-compose.yml

# Update docker-compose.yml RAG service (it's currently correct):
# CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

# Verify imports in main.py
grep "^from " main.py | sort | uniq
```

---

### 8. **Test Suite Missing/Incomplete** (CRITICAL for PR)

**Problem**: Test files reference fixtures that don't exist in `conftest.py`.

**Issues in tests**:
- `test_healthcheck.py:23` uses `@pytest.fixture` that references `mock_redis`, `mock_llm`, etc.
- `conftest.py` only defines `test_config`, `mock_llm`, `mock_embeddings`, `mock_vectorstore`
- Missing: `mock_redis`, `mock_crawler`, `mock_psutil`, `temp_library`, `temp_knowledge`, `temp_faiss_index`, `ryzen_env`

**Impact**: `pytest tests/` fails with fixture errors before any tests run.

**Fix**: Add missing fixtures to `conftest.py`:

```python
import pytest
from unittest.mock import Mock, MagicMock
from pathlib import Path
import os

@pytest.fixture
def mock_redis():
    """Mock Redis client."""
    mock = MagicMock()
    mock.ping = Mock(return_value=True)
    mock.get = Mock(return_value=None)
    mock.setex = Mock(return_value=True)
    mock.xadd = Mock(return_value=b'0-0')
    return mock

@pytest.fixture
def mock_crawler():
    """Mock CrawlModule/crawl4ai crawler."""
    mock = MagicMock()
    mock.warmup = Mock()
    mock.crawl = Mock(return_value=[])
    return mock

@pytest.fixture
def mock_psutil():
    """Mock psutil Process."""
    mock = MagicMock()
    memory_info = MagicMock()
    memory_info.rss = 4 * 1024 ** 3  # 4GB
    mock.memory_info = Mock(return_value=memory_info)
    return mock

@pytest.fixture
def temp_library(tmp_path):
    """Create temporary library directory structure."""
    lib_dir = tmp_path / "library"
    lib_dir.mkdir()
    for category in ["classics", "physics", "psychology"]:
        cat_dir = lib_dir / category
        cat_dir.mkdir()
        for i in range(5):
            (cat_dir / f"doc_{i:04d}.txt").write_text(f"Test document {i}")
    return lib_dir

@pytest.fixture
def temp_knowledge(tmp_path):
    """Create temporary knowledge directory with curator metadata."""
    know_dir = tmp_path / "knowledge"
    know_dir.mkdir()
    curator_dir = know_dir / "curator"
    curator_dir.mkdir()
    
    import toml
    metadata = {
        f"doc_{i:04d}": {
            "source": "test",
            "category": "test",
            "timestamp": "2025-10-18T00:00:00Z"
        }
        for i in range(5)
    }
    with open(curator_dir / "index.toml", "w") as f:
        toml.dump(metadata, f)
    
    return know_dir

@pytest.fixture
def temp_faiss_index(tmp_path):
    """Create temporary FAISS index directory."""
    index_dir = tmp_path / "faiss_index"
    index_dir.mkdir()
    (index_dir / "index.faiss").write_bytes(b"MOCK_INDEX")
    (index_dir / "index.pkl").write_bytes(b"MOCK_PKL")
    return index_dir

@pytest.fixture
def ryzen_env(monkeypatch):
    """Set Ryzen optimization environment variables."""
    env_vars = {
        "LLAMA_CPP_N_THREADS": "6",
        "LLAMA_CPP_F16_KV": "true",
        "OPENBLAS_CORETYPE": "ZEN",
        "LLAMA_CPP_USE_MLOCK": "true",
        "LLAMA_CPP_USE_MMAP": "true",
        "MEMORY_LIMIT_GB": "6.0",
        "CHAINLIT_NO_TELEMETRY": "true",
        "CRAWL4AI_NO_TELEMETRY": "true",
    }
    for key, value in env_vars.items():
        monkeypatch.setenv(key, value)
    return env_vars

@pytest.fixture(autouse=True)
def cleanup_environment():
    """Auto-cleanup after each test."""
    yield
    # pytest's tmp_path handles cleanup automatically
```

---

## II. MAJOR CODE QUALITY ISSUES

### 9. **`main.py` Missing Error Recovery** (MAJOR)

**Problem**: Global `llm` variable never handles reload/re-initialization on failure.

**Current** (lines 31-36):
```python
llm = None
embeddings = None
vectorstore = None
```

Then in `query_endpoint()` (line 178):
```python
if llm is None:
    logger.info("Lazy loading LLM...")
    llm = get_llm()
```

**Issue**: If `get_llm()` fails, `llm` stays `None`, but subsequent calls retry indefinitely without backoff.

**Fix**: Add retry logic with backoff:

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10))
def _get_llm_with_retry():
    global llm
    if llm is None:
        llm = get_llm()
    return llm

# In query_endpoint:
try:
    llm = _get_llm_with_retry()
except Exception as e:
    logger.error(f"LLM initialization failed: {e}")
    raise HTTPException(status_code=503, detail="LLM unavailable")
```

---

### 10. **`app.py` Non-blocking Subprocess Issue** (MAJOR)

**Problem**: Line 172-190 dispatches `/curate` command as subprocess but doesn't handle completion.

**Current**:
```python
proc = Popen(
    ['python3', '/app/XNAi_rag_app/crawl.py', '--curate', source, ...],
    stdout=DEVNULL,
    stderr=PIPE,
    start_new_session=True
)
active_curations[curation_id] = {
    'pid': proc.pid,
    ...
}
```

**Issues**:
- Process detached (won't be reaped if parent dies)
- No cleanup of zombie processes
- `active_curations` dict grows unbounded
- No error capture from stderr

**Fix**:

```python
import asyncio
from threading import Thread

def _curation_worker(source: str, category: str, query: str, curation_id: str):
    """Worker thread for curation with error handling."""
    try:
        proc = Popen(
            ['python3', '/app/XNAi_rag_app/crawl.py', 
             '--curate', source, '-c', category, '-q', query, '--embed'],
            stdout=PIPE,
            stderr=PIPE,
            text=True
        )
        
        stdout, stderr = proc.communicate(timeout=3600)
        
        if proc.returncode != 0:
            logger.error(f"Curation {curation_id} failed: {stderr}")
            active_curations[curation_id]['status'] = 'failed'
            active_curations[curation_id]['error'] = stderr[:500]
        else:
            logger.info(f"Curation {curation_id} completed")
            active_curations[curation_id]['status'] = 'completed'
            
    except subprocess.TimeoutExpired:
        proc.kill()
        logger.error(f"Curation {curation_id} timeout")
        active_curations[curation_id]['status'] = 'timeout'
    except Exception as e:
        logger.error(f"Curation {curation_id} error: {e}")
        active_curations[curation_id]['status'] = 'error'
        active_curations[curation_id]['error'] = str(e)

# In handle_command:
thread = Thread(target=_curation_worker, args=(source, category, query, curation_id), daemon=False)
thread.start()

return f"Curation started (tracking ID: {curation_id})"
```

---

### 11. **`crawl.py` Allowlist Enforcement is Regex, Not Glob** (MAJOR)

**Problem**: `is_allowed_url()` uses regex but `allowlist.txt` uses shell globs. Mismatch.

**Current** (line 98-106):
```python
def is_allowed_url(url: str, allowlist: List[str]) -> bool:
    for pattern in allowlist:
        regex_pattern = pattern.replace('.', r'\.').replace('*', '.*')
        if re.search(regex_pattern, url):
            return True
    return False
```

**Issue**: Pattern `*.gutenberg.org` converts to regex `.*\.gutenberg\.org`, which matches anywhere in URL. Should anchor to domain only.

**Fix**:
```python
def is_allowed_url(url: str, allowlist: List[str]) -> bool:
    """Validate URL against allowlist (glob patterns)."""
    from urllib.parse import urlparse
    parsed = urlparse(url)
    domain = parsed.netloc.lower()
    
    for pattern in allowlist:
        # Convert glob to regex, anchored to domain
        regex_pattern = pattern.lower().replace('.', r'\.').replace('*', '[^.]*')
        regex_pattern = f"^{regex_pattern}$"
        
        if re.match(regex_pattern, domain):
            return True
    
    logger.warning(f"URL domain not in allowlist: {domain}")
    return False
```

---

### 12. **`ingest_library.py` Missing Batch Processing** (MAJOR)

**Problem**: `ingest_library()` loads all documents into memory before processing.

**Current**:
```python
def ingest_library(library_path: str, batch_size: int = 100, force: bool = False) -> int:
    vectorstore = get_vectorstore()
    documents = []
    for file_path in tqdm(Path(library_path).rglob("*.txt")):
        with open(file_path, 'r') as f:
            documents.append(Document(...))
        if len(documents) >= batch_size:
            vectorstore.add_documents(documents)
            documents = []
```

**Issue**: Doesn't call `vectorstore.save_local()` after each batch. If process crashes mid-ingestion, all progress is lost.

**Fix**:
```python
def ingest_library(library_path: str, batch_size: int = 100, force: bool = False) -> int:
    """Ingest library with checkpointing."""
    vectorstore = get_vectorstore()
    documents = []
    saved_count = 0
    
    for file_path in tqdm(Path(library_path).rglob("*.txt")):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                documents.append(Document(
                    page_content=content,
                    metadata={"path": str(file_path)}
                ))
            
            if len(documents) >= batch_size:
                vectorstore.add_documents(documents)
                vectorstore.save_local(INDEX_PATH)  # Checkpoint
                saved_count += len(documents)
                documents = []
                logger.info(f"Checkpointed: {saved_count} docs")
                
        except Exception as e:
            logger.error(f"Error processing {file_path}: {e}")
            continue
    
    # Final batch
    if documents:
        vectorstore.add_documents(documents)
        vectorstore.save_local(INDEX_PATH)
        saved_count += len(documents)
    
    return saved_count
```

---

## III. TESTING COVERAGE GAPS

### 13. **No Performance Benchmarking Tests** (MAJOR)

**Missing**: Tests for token rate (15-25 tok/s), memory (<6GB), latency (<1s).

**Add to `test_integration.py`**:

```python
@pytest.mark.benchmark
@pytest.mark.slow
def test_token_rate_meets_target(mock_llm, monkeypatch):
    """Verify token generation meets 15-25 tok/s target."""
    import time
    
    # Mock LLM to generate 100 tokens
    mock_llm.invoke.return_value = " ".join(["token"] * 100)
    
    with patch('dependencies.get_llm', return_value=mock_llm):
        from main import app
        from fastapi.testclient import TestClient
        
        client = TestClient(app)
        
        start = time.time()
        response = client.post('/query', json={'query': 'test'})
        duration = time.time() - start
        
        # Rough estimate: 100 tokens in duration
        token_rate = 100 / duration
        
        assert 15 <= token_rate <= 30, f"Token rate {token_rate} outside target"

@pytest.mark.benchmark
def test_memory_under_limit(mock_llm, mock_vectorstore, monkeypatch):
    """Verify memory stays under 6GB target."""
    import psutil
    
    process = psutil.Process()
    mem_before = process.memory_info().rss / (1024**3)
    
    with patch('dependencies.get_llm', return_value=mock_llm), \
         patch('dependencies.get_vectorstore', return_value=mock_vectorstore):
        from main import app
        from fastapi.testclient import TestClient
        
        client = TestClient(app)
        for _ in range(10):
            client.post('/query', json={'query': f'test query'})
    
    mem_after = process.memory_info().rss / (1024**3)
    
    assert mem_after < 6.0, f"Memory {mem_after}GB exceeds 6GB limit"
```

---

### 14. **No Security Tests** (MAJOR)

**Missing**: Allowlist validation, script sanitization, telemetry disable verification.

**Add**:

```python
@pytest.mark.security
def test_allowlist_blocks_malicious_urls(test_config):
    """Verify allowlist rejects non-whitelisted URLs."""
    from crawl import is_allowed_url
    
    allowlist = ["*.gutenberg.org", "*.arxiv.org"]
    
    assert is_allowed_url("https://www.gutenberg.org/ebooks/1", allowlist)
    assert not is_allowed_url("https://evil.com", allowlist)
    assert not is_allowed_url("https://fake-gutenberg.org", allowlist)

@pytest.mark.security
def test_script_sanitization(test_config):
    """Verify script tags are removed."""
    from crawl import sanitize_content
    
    malicious = "<script>alert('xss')</script><p>Clean</p>"
    clean = sanitize_content(malicious)
    
    assert "<script>" not in clean
    assert "Clean" in clean

@pytest.mark.security
def test_telemetry_disabled(monkeypatch):
    """Verify all 8 telemetry disables are set."""
    telemetry_vars = [
        'CHAINLIT_NO_TELEMETRY',
        'CRAWL4AI_NO_TELEMETRY',
        'LANGCHAIN_TRACING_V2',
        'PYDANTIC_NO_TELEMETRY',
        'FASTAPI_NO_TELEMETRY',
    ]
    
    for var in telemetry_vars:
        assert os.getenv(var) in ['true', 'false', None], f"{var} not set"
```

---

## IV. DOCUMENTATION GAPS

### 15. **No DEPLOYMENT.md** (MINOR but Required for PR)

Create `DEPLOYMENT.md`:

```markdown
# Deployment Guide - Xoe-NovAi Phase 1 v0.1.2

## Pre-Deployment Checks

### 1. Configuration Validation
\`\`\`bash
bash validate-config.sh
python3 -c "import toml; toml.load('config.toml')"
grep -c "^[A-Z_]*=" .env | grep 197
\`\`\`

### 2. Import Validation
\`\`\`bash
python3 app/XNAi_rag_app/verify_imports.py
python3 -m py_compile app/XNAi_rag_app/*.py
\`\`\`

### 3. Test Suite
\`\`\`bash
pytest app/XNAi_rag_app/tests/ -v --cov --cov-report=html
# Target: >90% coverage
\`\`\`

## Deployment

### 1. Build
\`\`\`bash
docker compose build --no-cache
\`\`\`

### 2. Deploy
\`\`\`bash
docker compose up -d
sleep 90  # Wait for health checks
\`\`\`

### 3. Post-Deployment Verification
\`\`\`bash
curl http://localhost:8000/health | jq .status
curl http://localhost:8001/health
docker compose ps
\`\`\`

## Performance Validation

- Memory: `docker stats --no-stream | grep xnai`
- Token Rate: See metrics at `http://localhost:8002/metrics`
- Latency: `curl -w "@curl-format.txt" http://localhost:8000/query`
```

---

## V. ACTION PLAN FOR PR-READINESS

### Phase 1: Fix Critical Blockers (2-3 days)

1. Fix `config.toml` TOML parse error (add `#`)
2. Fix `logging_config.py` line 410 structure
3. Fix import paths in all modules
4. Add missing `chainlit_app.py`
5. Add `get_curator` to `healthcheck.py` imports
6. Add missing test fixtures in `conftest.py`

**Validation**: All tests run without import errors

### Phase 2: Fix Major Issues (2-3 days)

7. Implement retry logic in `main.py`
8. Fix subprocess handling in `app.py`
9. Fix allowlist URL validation in `crawl.py`
10. Add checkpointing to `ingest_library.py`
11. Fix `get_curator()` return type documentation

**Validation**: `pytest --cov` shows >90% coverage

### Phase 3: Add Missing Tests & Docs (2-3 days)

12. Add performance benchmark tests
13. Add security tests
14. Add DEPLOYMENT.md
15. Update README with troubleshooting

**Validation**: All tests pass, docs complete

---

## VI. CHECKLIST FOR PR SUBMISSION

- [ ] All Python files compile without syntax errors (`python3 -m py_compile *.py`)
- [ ] All imports resolve (`pytest --collect-only` succeeds)
- [ ] `config.toml` valid TOML (`python3 -c "import toml; toml.load('config.toml')"`)
- [ ] Tests >90% coverage (`pytest --cov`)
- [ ] Docker build succeeds (`docker compose build --no-cache`)
- [ ] Services start and reach health status
- [ ] No warnings in `docker compose config`
- [ ] Performance targets met (15-25 tok/s, <6GB memory, <1s latency)
- [ ] Security tests pass (allowlist, sanitization)
- [ ] DEPLOYMENT.md complete
- [ ] README updated with known issues & workarounds

---

## VII. ESTIMATED TIMELINE

| Phase | Duration | Effort |
|-------|----------|--------|
| Critical blockers | 2-3 days | ~16 hours |
| Major issues | 2-3 days | ~12 hours |
| Tests & docs | 2-3 days | ~8 hours |
| Regression testing | 1-2 days | ~6 hours |
| **Total** | **7-10 days** | **~42 hours** |

**Recommendation**: Assign 1 developer full-time or 2 developers part-time for this sprint.